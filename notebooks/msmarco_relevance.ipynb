{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_nli_classifier(model_name=\"microsoft/deberta-v2-xlarge-mnli\"):\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model_name, batch_size=256)    \n",
    "    return classifier\n",
    "    \n",
    "@lru_cache(maxsize=1)\n",
    "def load_msmarco_dataset():\n",
    "    msmarco_ds = load_dataset(\"ms_marco\", \"v2.1\", split=\"train\", streaming=True)\n",
    "    return msmarco_ds\n",
    "\n",
    "def test_entailment():\n",
    "    # simple test case\n",
    "    query = \"What was Apple's revenue in Q2 2025?\"\n",
    "    premise = \"Apple reported $119.6 billion in revenue for Q2 2025.\"\n",
    "    \n",
    "    # test cases: entailment, contradiction, neutral\n",
    "    hypothesis1 = \"In the second quarter of 2025, Apple posted revenue of $119.6 billion, beating analyst expectations.\"\n",
    "    hypothesis2 = \"Apple's Q2 2025 revenue was only $90 billion, which was below expectations.\"\n",
    "    hypothesis3 = \"Apple launched the Vision Pro headset in 2024 as part of its expansion into spatial computing.\"\n",
    "    \n",
    "    classifier = load_nli_classifier()\n",
    "    result = classifier(premise, [hypothesis1, hypothesis2, hypothesis3])\n",
    "    scores = result['scores']\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def annotate_msmarco(how_many=3):\n",
    "    annotations = list()\n",
    "    queries = list()\n",
    "\n",
    "    classifier = load_nli_classifier()\n",
    "    msmarco_ds = load_msmarco_dataset()\n",
    "    \n",
    "    for i, e in enumerate(tqdm(msmarco_ds, total=how_many, desc=\"Annotating MS MARCO\")):\n",
    "        if len(queries) > how_many:\n",
    "            print(\"dataset limit reached\")\n",
    "            break\n",
    "        \n",
    "        query_id, query = e['query_id'], e['query']\n",
    "\n",
    "        answer = next(iter(e['answers']), None)\n",
    "        well_formed = next(iter(e['wellFormedAnswers']), None)\n",
    "        premise = well_formed if well_formed else answer\n",
    "        queries.append({'query_id': query_id, 'query': query, 'answer': bool(answer), 'well_formed': bool(well_formed)})\n",
    "        \n",
    "        # make sure the selected passage is first\n",
    "        pairs = zip(e['passages']['is_selected'], e['passages']['passage_text'])\n",
    "        passages = [(s, p) for s, p in pairs]\n",
    "        passages.sort(reverse=True)\n",
    "        \n",
    "        if passages[0][0] != 1:\n",
    "            continue\n",
    "        \n",
    "        if not premise:\n",
    "            premise = query\n",
    "        \n",
    "        with torch.autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "            result = classifier(premise, [p[1] for p in passages])\n",
    "        \n",
    "        scores = result['scores']\n",
    "    \n",
    "        for score, (selected, passage_text) in zip(scores, passages):\n",
    "            annotations.append({\n",
    "                'query_id': query_id,\n",
    "                'passage': passage_text,\n",
    "                'selected': selected == 1,\n",
    "                'score': score,\n",
    "            })\n",
    "        \n",
    "    return queries, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcd73e8-b755-4a53-8bba-8e60ae6d20cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating MS MARCO:   5%|██████▌                                                                                                                                        | 460/10000 [07:06<2:27:20,  1.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m queries, annotations = \u001b[43mannotate_msmarco\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow_many\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m q_df = pd.DataFrame(queries)\n\u001b[32m      3\u001b[39m a_df = pd.DataFrame(annotations)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mannotate_msmarco\u001b[39m\u001b[34m(how_many)\u001b[39m\n\u001b[32m     31\u001b[39m     premise = query\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.float16):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     result = \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpremise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m scores = result[\u001b[33m'\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m score, (selected, passage_text) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(scores, passages):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llmjj/.venv/lib/python3.13/site-packages/transformers/pipelines/zero_shot_classification.py:209\u001b[39m, in \u001b[36mZeroShotClassificationPipeline.__call__\u001b[39m\u001b[34m(self, sequences, *args, **kwargs)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llmjj/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1459\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llmjj/.venv/lib/python3.13/site-packages/transformers/pipelines/pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llmjj/.venv/lib/python3.13/site-packages/transformers/pipelines/pt_utils.py:271\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    268\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llmjj/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1375\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m   1374\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._forward(model_inputs, **forward_params)\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFramework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.framework\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llmjj/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1275\u001b[39m, in \u001b[36mPipeline._ensure_tensor_on_device\u001b[39m\u001b[34m(self, inputs, device)\u001b[39m\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[32m   1272\u001b[39m         {name: \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m   1273\u001b[39m     )\n\u001b[32m   1274\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, UserDict):\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDict({name: \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs.items()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llmjj/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1283\u001b[39m, in \u001b[36mPipeline._ensure_tensor_on_device\u001b[39m\u001b[34m(self, inputs, device)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(item, device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch.Tensor):\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "queries, annotations = annotate_msmarco(how_many=10000)\n",
    "q_df = pd.DataFrame(queries)\n",
    "a_df = pd.DataFrame(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "well_formed_df = df[df.well_formed]\n",
    "well_formed_selected_df = df[df.well_formed & (df.selected == True)]\n",
    "answered_df = df[df.answer.astype(bool) & ~df.well_formed]\n",
    "answered_selected_df = df[df.answer.astype(bool) & ~df.well_formed & (df.selected == True)]\n",
    "answered_notselected_df = df[df.answer.astype(bool) & ~df.well_formed & (df.selected == False)]\n",
    "\n",
    "not_selected_df = df[df.selected == False]\n",
    "\n",
    "# easier to add a group and feed the input to seaborn\n",
    "plot_df = pd.concat([\n",
    "    answered_df.assign(group=\"Answered\"),\n",
    "    answered_selected_df.assign(group=\"Answered Selected\"),\n",
    "    answered_notselected_df.assign(group=\"Answered Not Selected\"),\n",
    "    well_formed_df.assign(group=\"Well Formed\"),\n",
    "    well_formed_selected_df.assign(group=\"Well Formed Selected\"),\n",
    "    not_selected_df.assign(group=\"Not Selected\")\n",
    "]).reset_index()\n",
    "\n",
    "\n",
    "group_stats = plot_df.groupby(\"group\")[\"score\"].agg([\"count\", \"median\", \"mean\", \"std\"])\n",
    "print(group_stats)\n",
    "\n",
    "counts = plot_df[\"group\"].value_counts()\n",
    "plot_df[\"group_label\"] = plot_df[\"group\"].apply(lambda g: f\"{g}\\n(n={counts[g]})\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.violinplot(data=plot_df, x=\"group_label\", y=\"score\", inner=None)\n",
    "\n",
    "\n",
    "means_str = \"; \".join(\n",
    "    f\"{g}: μ={group_stats.loc[g,'mean']:.2f}\" \n",
    "    for g in group_stats.index\n",
    ")\n",
    "\n",
    "plt.title(f\"Score Distributions by Group\\n{means_str}\")\n",
    "\n",
    "plt.xlabel(\"Group\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"msmarco_score_distribution.png.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base stats\n",
    "group_stats = plot_df.groupby(\"group\")[\"score\"].agg([\"count\", \"median\", \"mean\", \"std\"])\n",
    "\n",
    "# number of rows per query_id where score > 0.4, averaged over all rows\n",
    "rows_per_qid_over04 = (\n",
    "    plot_df.groupby([\"group\", \"query_id\"])\n",
    "    .apply(lambda g: (g[\"score\"] > 0.4).sum())   # count rows > 0.4 for each query_id\n",
    "    .groupby(\"group\")\n",
    "    .mean()\n",
    "    .rename(\"avg_rows_gt_0.4_per_qid\")\n",
    ")\n",
    "\n",
    "# proportion of all rows with score > 0.4\n",
    "prop_over04 = (\n",
    "    plot_df.groupby(\"group\")[\"score\"]\n",
    "    .apply(lambda s: (s > 0.4).mean())\n",
    "    .rename(\"prop_rows_gt_0.4\")\n",
    ")\n",
    "\n",
    "# join into stats\n",
    "group_stats = group_stats.join([rows_per_qid_over04, prop_over04])\n",
    "print(group_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
