{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_nli_classifier(model_name=\"microsoft/deberta-v2-xlarge-mnli\"):\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model_name, batch_size=256)    \n",
    "    return classifier\n",
    "    \n",
    "@lru_cache(maxsize=1)\n",
    "def load_msmarco_dataset():\n",
    "    msmarco_ds = load_dataset(\"ms_marco\", \"v2.1\", split=\"train\", streaming=True)\n",
    "    return msmarco_ds\n",
    "\n",
    "def test_entailment():\n",
    "    # simple test case\n",
    "    query = \"What was Apple's revenue in Q2 2025?\"\n",
    "    premise = \"Apple reported $119.6 billion in revenue for Q2 2025.\"\n",
    "    \n",
    "    # test cases: entailment, contradiction, neutral\n",
    "    hypothesis1 = \"In the second quarter of 2025, Apple posted revenue of $119.6 billion, beating analyst expectations.\"\n",
    "    hypothesis2 = \"Apple's Q2 2025 revenue was only $90 billion, which was below expectations.\"\n",
    "    hypothesis3 = \"Apple launched the Vision Pro headset in 2024 as part of its expansion into spatial computing.\"\n",
    "    \n",
    "    classifier = load_nli_classifier()\n",
    "    result = classifier(premise, [hypothesis1, hypothesis2, hypothesis3])\n",
    "    scores = result['scores']\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def annotate_msmarco(how_many=3):\n",
    "    annotations = list()\n",
    "    queries = list()\n",
    "\n",
    "    classifier = load_nli_classifier()\n",
    "    msmarco_ds = load_msmarco_dataset()\n",
    "    \n",
    "    for i, e in enumerate(tqdm(msmarco_ds, total=how_many, desc=\"Annotating MS MARCO\")):\n",
    "        if len(queries) > how_many:\n",
    "            print(\"dataset limit reached\")\n",
    "            break\n",
    "        \n",
    "        query_id, query = e['query_id'], e['query']\n",
    "\n",
    "        answer = next(iter(e['answers']), None)\n",
    "        well_formed = next(iter(e['wellFormedAnswers']), None)\n",
    "        premise = well_formed if well_formed else answer\n",
    "        queries.append({'query_id': query_id, 'query': query, 'answer': bool(answer), 'well_formed': bool(well_formed)})\n",
    "        \n",
    "        # make sure the selected passage is first\n",
    "        pairs = zip(e['passages']['is_selected'], e['passages']['passage_text'])\n",
    "        passages = [(s, p) for s, p in pairs]\n",
    "        passages.sort(reverse=True)\n",
    "        \n",
    "        if passages[0][0] != 1:\n",
    "            continue\n",
    "        \n",
    "        if not premise:\n",
    "            premise = query\n",
    "        \n",
    "        with torch.autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "            result = classifier(premise, [p[1] for p in passages])\n",
    "        \n",
    "        scores = result['scores']\n",
    "    \n",
    "        for score, (selected, passage_text) in zip(scores, passages):\n",
    "            annotations.append({\n",
    "                'query_id': query_id,\n",
    "                'passage': passage_text,\n",
    "                'selected': selected == 1,\n",
    "                'score': score,\n",
    "            })\n",
    "        \n",
    "    return queries, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, annotations = annotate_msmarco(how_many=10000)\n",
    "q_df = pd.DataFrame(queries)\n",
    "a_df = pd.DataFrame(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "well_formed_df = df[df.well_formed]\n",
    "well_formed_selected_df = df[df.well_formed & (df.selected == True)]\n",
    "answered_df = df[df.answer.astype(bool) & ~df.well_formed]\n",
    "answered_selected_df = df[df.answer.astype(bool) & ~df.well_formed & (df.selected == True)]\n",
    "answered_notselected_df = df[df.answer.astype(bool) & ~df.well_formed & (df.selected == False)]\n",
    "\n",
    "not_selected_df = df[df.selected == False]\n",
    "\n",
    "# easier to add a group and feed the input to seaborn\n",
    "plot_df = pd.concat([\n",
    "    answered_df.assign(group=\"Answered\"),\n",
    "    answered_selected_df.assign(group=\"Answered Selected\"),\n",
    "    answered_notselected_df.assign(group=\"Answered Not Selected\"),\n",
    "    well_formed_df.assign(group=\"Well Formed\"),\n",
    "    well_formed_selected_df.assign(group=\"Well Formed Selected\"),\n",
    "    not_selected_df.assign(group=\"Not Selected\")\n",
    "]).reset_index()\n",
    "\n",
    "\n",
    "group_stats = plot_df.groupby(\"group\")[\"score\"].agg([\"count\", \"median\", \"mean\", \"std\"])\n",
    "print(group_stats)\n",
    "\n",
    "counts = plot_df[\"group\"].value_counts()\n",
    "plot_df[\"group_label\"] = plot_df[\"group\"].apply(lambda g: f\"{g}\\n(n={counts[g]})\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.violinplot(data=plot_df, x=\"group_label\", y=\"score\", inner=None)\n",
    "\n",
    "\n",
    "means_str = \"; \".join(\n",
    "    f\"{g}: Î¼={group_stats.loc[g,'mean']:.2f}\" \n",
    "    for g in group_stats.index\n",
    ")\n",
    "\n",
    "plt.title(f\"Score Distributions by Group\\n{means_str}\")\n",
    "\n",
    "plt.xlabel(\"Group\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"msmarco_score_distribution.png.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base stats\n",
    "group_stats = plot_df.groupby(\"group\")[\"score\"].agg([\"count\", \"median\", \"mean\", \"std\"])\n",
    "\n",
    "# number of rows per query_id where score > 0.4, averaged over all rows\n",
    "rows_per_qid_over04 = (\n",
    "    plot_df.groupby([\"group\", \"query_id\"])\n",
    "    .apply(lambda g: (g[\"score\"] > 0.4).sum())   # count rows > 0.4 for each query_id\n",
    "    .groupby(\"group\")\n",
    "    .mean()\n",
    "    .rename(\"avg_rows_gt_0.4_per_qid\")\n",
    ")\n",
    "\n",
    "# proportion of all rows with score > 0.4\n",
    "prop_over04 = (\n",
    "    plot_df.groupby(\"group\")[\"score\"]\n",
    "    .apply(lambda s: (s > 0.4).mean())\n",
    "    .rename(\"prop_rows_gt_0.4\")\n",
    ")\n",
    "\n",
    "# join into stats\n",
    "group_stats = group_stats.join([rows_per_qid_over04, prop_over04])\n",
    "print(group_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
